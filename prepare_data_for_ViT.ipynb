{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511d1b23-f6fb-425d-b105-221c4057c58e",
   "metadata": {},
   "source": [
    "## Prepare Data for ViT-based estimation method\n",
    "\n",
    "used code in ViT-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "031b67d2-84cd-4da1-a473-d3fddec565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from tools.utils_at import *\n",
    "from mmdet.models.utils import transform_tensors_to_list\n",
    "from mmdet.structures.bbox import (bbox_cxcywh_to_xyxy, bbox_overlaps,\n",
    "                                   bbox_xyxy_to_cxcywh)\n",
    "from torchvision.ops.boxes import box_area\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe88c1f-162d-4749-bc25-03e22536074f",
   "metadata": {},
   "source": [
    "# function\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou, union\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "    The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "    and M = len(boxes2)\n",
    "    \"\"\"\n",
    "    # degenerate boxes gives inf / nan results\n",
    "    # so do an early check\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / (area + 1e-6)\n",
    "    \n",
    "def hungarian_matching(out_logits, out_boxes, tgt_ids, tgt_bbox, cost_class_weight = 1.0, cost_bbox_weight = 5.0, cost_giou_weight = 2.0, focal_alpha = 0.25):\n",
    "    \"\"\" Performs the matching\n",
    "    \"\"\"\n",
    "    \n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    num_queries = out_logits.shape[0]\n",
    "    out_prob = out_logits.softmax(dim=1)  # [num_queries, num_classes]\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_boxes, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(bbox_cxcywh_to_xyxy(out_boxes), bbox_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox_weight + cost_class * cost_class_weight + cost_giou * cost_giou_weight\n",
    "    C = C.view(num_queries, -1)\n",
    "    result = torch.argmin(C, axis=1)\n",
    "    return result\n",
    "\n",
    "def get_indexes(out_logits):\n",
    "    prob = out_logits.softmax(dim=1)\n",
    "    prob, _ = prob.max(dim=1)\n",
    "    select_mask = prob > score_threshold\n",
    "    if sum(select_mask) == 0:\n",
    "        score, indexes = prob.topk(10)\n",
    "        indexes,_ = torch.sort(indexes, dim=0)\n",
    "        print(f\"Cannot find detected objects with score larger than {score_threshold}\")\n",
    "    else:\n",
    "        indexes = select_mask.nonzero().reshape(-1)\n",
    "    return indexes\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "    return loss\n",
    "    \n",
    "def compute_loss(out_logits, out_boxes, tgt_ids, tgt_bbox, matched_target_indexes, cls_loss_coef = 1.0, bbox_loss_coef = 5.0, giou_loss_coef = 2.0):\n",
    "    if matched_target_indexes == None:\n",
    "        target_classes_onehot = torch.zeros([out_logits.shape[0], out_logits.shape[1]],dtype=out_logits.dtype, layout=out_logits.layout, \n",
    "                                        device=out_logits.device)\n",
    "        loss_ce = sigmoid_focal_loss(out_logits, target_classes_onehot)\n",
    "        loss_ce = loss_ce.sum(axis=1)\n",
    "        loss = loss_ce * cls_loss_coef\n",
    "        return loss, loss_ce, torch.zeros(loss.shape), torch.zeros(loss.shape)\n",
    "    cls_loss_coef = 1.0\n",
    "    bbox_loss_coef = 5.0\n",
    "    giou_loss_coef = 2.0\n",
    "    target_boxes = tgt_bbox[matched_target_indexes]\n",
    "    loss_bbox = F.l1_loss(out_boxes, target_boxes, reduction='none') # [num_queries, 4]\n",
    "    loss_bbox = loss_bbox.mean(axis=1) # [num_queries]\n",
    "    loss_giou = 1 - torch.diag(generalized_box_iou(bbox_cxcywh_to_xyxy(out_boxes),\n",
    "                bbox_cxcywh_to_xyxy(target_boxes))) # [num_queries]\n",
    "    target_classes_onehot = torch.zeros([out_logits.shape[0], out_logits.shape[1]],dtype=out_logits.dtype, layout=out_logits.layout, \n",
    "                                        device=out_logits.device)\n",
    "    target_labels = torch.tensor(tgt_ids)[matched_target_indexes]\n",
    "    target_classes_onehot.scatter_(1, target_labels.unsqueeze(-1), 1)\n",
    "    loss_ce = sigmoid_focal_loss(out_logits, target_classes_onehot)\n",
    "    loss_ce = loss_ce.sum(axis=1)\n",
    "    loss = loss_ce * cls_loss_coef + loss_bbox * bbox_loss_coef + loss_giou * giou_loss_coef\n",
    "    return loss, loss_ce, loss_bbox, loss_giou\n",
    "\n",
    "def generate_feature_annotation(num, count):\n",
    "    file_path = data_path + str(num) + \".json\"\n",
    "    output_data = read_json_results(file_path)\n",
    "    feature = np.array(output_data['feature'][5][0])\n",
    "    out_logits = torch.FloatTensor(output_data['pred_logits'][5][0])\n",
    "    out_boxes = torch.FloatTensor(output_data['pred_boxes'][5][0])\n",
    "    target_labels = output_data['gt_labels']\n",
    "    target_boxes = torch.FloatTensor(output_data['gt_boxes'])\n",
    "    if len(target_boxes) == 0:\n",
    "        return False\n",
    "    target_boxes = bbox_xyxy_to_cxcywh(target_boxes)\n",
    "    img_h, img_w = output_data['img_metas'][0]['img_shape']\n",
    "    factors = output_data['img_metas'][0]['scale_factor']\n",
    "    shapes = torch.FloatTensor([img_h, img_w, img_h, img_w])\n",
    "    target_boxes = target_boxes / shapes\n",
    "    out_logits = out_logits[:, 0:80] # for detr & coco\n",
    "    indexes = get_indexes(out_logits)\n",
    "    out_logits = out_logits[indexes]\n",
    "    out_boxes = out_boxes[indexes]\n",
    "    gt_indexes = hungarian_matching(out_logits, out_boxes, target_labels, target_boxes)\n",
    "    loss, loss_ce, loss_bbox, loss_giou = compute_loss(out_logits, out_boxes, target_labels, target_boxes, gt_indexes)\n",
    "    np_write(feature, feature_path + str(count) + \".npy\")\n",
    "    json_data = {}\n",
    "    json_data['loss'] = transform_tensors_to_list(loss)\n",
    "    json_data['loss_ce'] = transform_tensors_to_list(loss_ce)\n",
    "    json_data['loss_bbox'] = transform_tensors_to_list(loss_bbox)\n",
    "    json_data['loss_giou'] = transform_tensors_to_list(loss_giou)\n",
    "    json_data['index'] = transform_tensors_to_list(indexes)\n",
    "    json_data['gt_indexes'] = transform_tensors_to_list(gt_indexes)\n",
    "    json_data['output_file_num'] = num\n",
    "    write_json_results(json_data, annotation_path + str(count) + \".json\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbecfaad-ecc2-412c-a612-57430aae0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./pro_data/detr/\"\n",
    "data_path = base_path + split + \"/outputs/\"\n",
    "feature_path = base_path + split + \"/feature/\"\n",
    "annotation_path = base_path + split + \"/annotation/\"\n",
    "create_folder_if_not_exists(feature_path)\n",
    "create_folder_if_not_exists(annotation_path)\n",
    "score_threshold = 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f42f5-7f0c-4668-a9ee-2cc7c4d0e21f",
   "metadata": {},
   "source": [
    "### Generate feature and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e533913-9d46-40a7-a552-b4c0ebe2b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_folder = os.listdir(data_path)\n",
    "num_files = len(files_in_folder) - 1\n",
    "count = 0\n",
    "for num in range(num_files):\n",
    "    flag = generate_feature_annotation(num, count)\n",
    "    if flag:\n",
    "        count += 1\n",
    "    if (num+1)%1000 == 0:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8661b-2715-4caf-ac2a-a16572beb4fd",
   "metadata": {},
   "source": [
    "### Generate image_true_losses and region_true_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09c7768f-df14-488b-a7df-d9da684aa19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_folder = os.listdir(annotation_path)\n",
    "nums = len(files_in_folder) - 1\n",
    "img_loss = np.zeros(nums)\n",
    "region_loss = []\n",
    "for i in range(nums):\n",
    "    path = annotation_path + str(i) + \".json\"\n",
    "    json_data = read_json_results(path)\n",
    "    losses = np.array(json_data['loss'])\n",
    "    img_loss[i] = losses.mean()\n",
    "    region_loss.extend(json_data['loss'])\n",
    "np_write(img_loss, base_path + split + \"/image_true_losses.npy\")\n",
    "np_write(np.array(region_loss), base_path + split + \"/region_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eed780e8-4a89-4aab-8c5d-facb66b03a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.308374007542928 34.88671875\n",
      "0.07327228039503098 37.063392639160156\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "image_loss = np_read(base_path + split + \"/image_true_losses.npy\")\n",
    "print(image_loss.min(), image_loss.max())\n",
    "region_loss = np_read(base_path + split + \"/region_true_losses.npy\")\n",
    "print(region_loss.min(), region_loss.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d354ddf6-233e-41eb-a439-10d40a2f9cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59ab38a0-2ede-412a-8e0f-90b52ae4176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 25\n",
    "file_path = data_path + str(num) + \".json\"\n",
    "output_data = read_json_results(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf74b5-5649-4b3c-8b60-649f40fbb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.array(output_data['feature'][5][0])\n",
    "out_logits = torch.FloatTensor(output_data['pred_logits'][5][0])\n",
    "out_boxes = torch.FloatTensor(output_data['pred_boxes'][5][0])\n",
    "target_labels = output_data['gt_labels']\n",
    "target_boxes = torch.FloatTensor(output_data['gt_boxes'])\n",
    "if len(target_boxes) > 0:\n",
    "    target_boxes = bbox_xyxy_to_cxcywh(target_boxes)\n",
    "img_h, img_w = output_data['img_metas'][0]['img_shape']\n",
    "factors = output_data['img_metas'][0]['scale_factor']\n",
    "shapes = torch.FloatTensor([img_h, img_w, img_h, img_w])\n",
    "target_boxes = target_boxes / shapes\n",
    "out_logits = out_logits[:, 0:80] # for detr & coco\n",
    "indexes = get_indexes(out_logits)\n",
    "out_logits = out_logits[indexes]\n",
    "out_boxes = out_boxes[indexes]\n",
    "gt_indexes = hungarian_matching(out_logits, out_boxes, target_labels, target_boxes)\n",
    "loss, loss_ce, loss_bbox, loss_giou = compute_loss(out_logits, out_boxes, target_labels, target_boxes, gt_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b14dc391-f5b5-47b2-a619-9358434237ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19431ff3-7f63-423d-853a-6d785d336d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_write(feature, feature_path + str(num) + \".npy\")\n",
    "json_data = {}\n",
    "json_data['loss'] = transform_tensors_to_list(loss)\n",
    "json_data['loss_ce'] = transform_tensors_to_list(loss_ce)\n",
    "json_data['loss_bbox'] = transform_tensors_to_list(loss_bbox)\n",
    "json_data['loss_giou'] = transform_tensors_to_list(loss_giou)\n",
    "json_data['index'] = transform_tensors_to_list(indexes)\n",
    "json_data['gt_indexes'] = transform_tensors_to_list(gt_indexes)\n",
    "json_data['output_file_num'] = num\n",
    "write_json_results(json_data, annotation_path + str(num) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed572c-971e-4aac-9265-c65f23072c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "365d9023-bafd-41d8-940f-a9d2c6f6cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = read_json_results(base_path + \"train\" + \"/annotation/\" + \"55397.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8101dce2-2795-4790-b76a-0f7531b74892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55894"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json['output_file_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36c83fdd-f3f6-465f-8cb7-6311dc92338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_loss = torch.tensor(json['loss'])\n",
    "img_loss = patch_loss.mean()\n",
    "patch_index = torch.tensor(json['index'])\n",
    "annotation = torch.full((100,), 255, dtype=patch_loss.dtype)\n",
    "annotation[patch_index] = patch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a585024f-b39c-40cc-ab4e-3dca1df38a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([255.0000, 255.0000,   7.3523, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000,   4.2870, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   5.5946, 255.0000,\n",
       "          8.4145,   5.0837, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000,   5.2658,   5.4688,   5.0664,\n",
       "        255.0000, 255.0000,   5.0659, 255.0000,   7.0068, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000,   6.5768, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   8.6178,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "          4.8617, 255.0000, 255.0000,   6.8172, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000,   6.6019,   6.3947,  11.1883, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   5.1456,\n",
       "        255.0000, 255.0000, 255.0000,   6.7193, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a28bf2d-76ef-42ea-b683-cf0ac871aa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 11, 19, 21, 22, 32, 33, 34, 37, 39, 51, 62, 70, 73, 78, 79, 80, 90,\n",
       "        94])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b99e9-3e52-4a04-b761-7e5b3ca8fbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
