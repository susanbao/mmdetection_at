{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511d1b23-f6fb-425d-b105-221c4057c58e",
   "metadata": {},
   "source": [
    "## Prepare Data for ViT-based estimation method\n",
    "\n",
    "used code in ViT-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "031b67d2-84cd-4da1-a473-d3fddec565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from tools.utils_at import *\n",
    "from torch import Tensor\n",
    "# from mmdet.models.utils import transform_tensors_to_list\n",
    "# from mmdet.structures.bbox.transforms import bbox_cxcywh_to_xyxy, bbox_xyxy_to_cxcywh\n",
    "# from mmdet.structures.bbox.bbox_overlaps import bbox_overlaps\n",
    "from torchvision.ops.boxes import box_area\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faf1ddc-27c1-4a7b-9c7c-a7f3f7a219e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_overlaps(bboxes1, bboxes2, mode='iou', is_aligned=False, eps=1e-6):\n",
    "    \"\"\"Calculate overlap between two set of boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode in ['iou', 'iof', 'giou'], f'Unsupported mode {mode}'\n",
    "    # Either the boxes are empty or the length of boxes' last dimension is 4\n",
    "    assert (bboxes1.size(-1) == 4 or bboxes1.size(0) == 0)\n",
    "    assert (bboxes2.size(-1) == 4 or bboxes2.size(0) == 0)\n",
    "\n",
    "    # Batch dim must be the same\n",
    "    # Batch dim: (B1, B2, ... Bn)\n",
    "    assert bboxes1.shape[:-2] == bboxes2.shape[:-2]\n",
    "    batch_shape = bboxes1.shape[:-2]\n",
    "\n",
    "    rows = bboxes1.size(-2)\n",
    "    cols = bboxes2.size(-2)\n",
    "    if is_aligned:\n",
    "        assert rows == cols\n",
    "\n",
    "    if rows * cols == 0:\n",
    "        if is_aligned:\n",
    "            return bboxes1.new(batch_shape + (rows, ))\n",
    "        else:\n",
    "            return bboxes1.new(batch_shape + (rows, cols))\n",
    "\n",
    "    area1 = (bboxes1[..., 2] - bboxes1[..., 0]) * (\n",
    "        bboxes1[..., 3] - bboxes1[..., 1])\n",
    "    area2 = (bboxes2[..., 2] - bboxes2[..., 0]) * (\n",
    "        bboxes2[..., 3] - bboxes2[..., 1])\n",
    "\n",
    "    if is_aligned:\n",
    "        lt = torch.max(bboxes1[..., :2], bboxes2[..., :2])  # [B, rows, 2]\n",
    "        rb = torch.min(bboxes1[..., 2:], bboxes2[..., 2:])  # [B, rows, 2]\n",
    "\n",
    "        wh = fp16_clamp(rb - lt, min=0)\n",
    "        overlap = wh[..., 0] * wh[..., 1]\n",
    "\n",
    "        if mode in ['iou', 'giou']:\n",
    "            union = area1 + area2 - overlap\n",
    "        else:\n",
    "            union = area1\n",
    "        if mode == 'giou':\n",
    "            enclosed_lt = torch.min(bboxes1[..., :2], bboxes2[..., :2])\n",
    "            enclosed_rb = torch.max(bboxes1[..., 2:], bboxes2[..., 2:])\n",
    "    else:\n",
    "        lt = torch.max(bboxes1[..., :, None, :2],\n",
    "                       bboxes2[..., None, :, :2])  # [B, rows, cols, 2]\n",
    "        rb = torch.min(bboxes1[..., :, None, 2:],\n",
    "                       bboxes2[..., None, :, 2:])  # [B, rows, cols, 2]\n",
    "\n",
    "        wh = fp16_clamp(rb - lt, min=0)\n",
    "        overlap = wh[..., 0] * wh[..., 1]\n",
    "\n",
    "        if mode in ['iou', 'giou']:\n",
    "            union = area1[..., None] + area2[..., None, :] - overlap\n",
    "        else:\n",
    "            union = area1[..., None]\n",
    "        if mode == 'giou':\n",
    "            enclosed_lt = torch.min(bboxes1[..., :, None, :2],\n",
    "                                    bboxes2[..., None, :, :2])\n",
    "            enclosed_rb = torch.max(bboxes1[..., :, None, 2:],\n",
    "                                    bboxes2[..., None, :, 2:])\n",
    "\n",
    "    eps = union.new_tensor([eps])\n",
    "    union = torch.max(union, eps)\n",
    "    ious = overlap / union\n",
    "    if mode in ['iou', 'iof']:\n",
    "        return ious\n",
    "    # calculate gious\n",
    "    enclose_wh = fp16_clamp(enclosed_rb - enclosed_lt, min=0)\n",
    "    enclose_area = enclose_wh[..., 0] * enclose_wh[..., 1]\n",
    "    enclose_area = torch.max(enclose_area, eps)\n",
    "    gious = ious - (enclose_area - union) / enclose_area\n",
    "    return gious\n",
    "    \n",
    "def bbox_xyxy_to_cxcywh(bbox: Tensor) -> Tensor:\n",
    "    \"\"\"Convert bbox coordinates from (x1, y1, x2, y2) to (cx, cy, w, h).\n",
    "\n",
    "    Args:\n",
    "        bbox (Tensor): Shape (n, 4) for bboxes.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Converted bboxes.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox.split((1, 1, 1, 1), dim=-1)\n",
    "    bbox_new = [(x1 + x2) / 2, (y1 + y2) / 2, (x2 - x1), (y2 - y1)]\n",
    "    return torch.cat(bbox_new, dim=-1)\n",
    "    \n",
    "def bbox_cxcywh_to_xyxy(bbox: Tensor) -> Tensor:\n",
    "    \"\"\"Convert bbox coordinates from (cx, cy, w, h) to (x1, y1, x2, y2).\n",
    "\n",
    "    Args:\n",
    "        bbox (Tensor): Shape (n, 4) for bboxes.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Converted bboxes.\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = bbox.split((1, 1, 1, 1), dim=-1)\n",
    "    bbox_new = [(cx - 0.5 * w), (cy - 0.5 * h), (cx + 0.5 * w), (cy + 0.5 * h)]\n",
    "    return torch.cat(bbox_new, dim=-1)\n",
    "    \n",
    "def transform_tensor_to_list(l):\n",
    "    return l.cpu().tolist()\n",
    "    \n",
    "def transform_tensors_to_list(l):\n",
    "    if torch.is_tensor(l):\n",
    "        return transform_tensor_to_list(l)\n",
    "    if isinstance(l, list):\n",
    "        r = []\n",
    "        for i in l:\n",
    "            r.append(transform_tensors_to_list(i))\n",
    "        return r\n",
    "    if isinstance(l, dict):\n",
    "        r = {}\n",
    "        for k,v in l.items():\n",
    "            r[k] = transform_tensors_to_list(v)\n",
    "        return r\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cfba35-48fb-4c18-b1ad-f90aa1e45641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    union = area1[:, None] + area2 - inter\n",
    "\n",
    "    iou = inter / (union + 1e-6)\n",
    "    return iou, union\n",
    "\n",
    "def generalized_box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Generalized IoU from https://giou.stanford.edu/\n",
    "\n",
    "    The boxes should be in [x0, y0, x1, y1] format\n",
    "\n",
    "    Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "    and M = len(boxes2)\n",
    "    \"\"\"\n",
    "    # degenerate boxes gives inf / nan results\n",
    "    # so do an early check\n",
    "    assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "    assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "\n",
    "    iou, union = box_iou(boxes1, boxes2)\n",
    "\n",
    "    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "    return iou - (area - union) / (area + 1e-6)\n",
    "    \n",
    "def hungarian_matching(out_logits, out_boxes, tgt_ids, tgt_bbox, cost_class_weight = 1.0, cost_bbox_weight = 5.0, cost_giou_weight = 2.0, focal_alpha = 0.25):\n",
    "    \"\"\" Performs the matching\n",
    "    \"\"\"\n",
    "    \n",
    "    # We flatten to compute the cost matrices in a batch\n",
    "    num_queries = out_logits.shape[0]\n",
    "    out_prob = out_logits.softmax(dim=1)  # [num_queries, num_classes]\n",
    "    \n",
    "    # Compute the classification cost.\n",
    "    alpha = focal_alpha\n",
    "    gamma = 2.0\n",
    "    neg_cost_class = (1 - alpha) * (out_prob ** gamma) * (-(1 - out_prob + 1e-8).log())\n",
    "    pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n",
    "    cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]\n",
    "    \n",
    "    # Compute the L1 cost between boxes\n",
    "    cost_bbox = torch.cdist(out_boxes, tgt_bbox, p=1)\n",
    "    \n",
    "    # Compute the giou cost betwen boxes            \n",
    "    cost_giou = -generalized_box_iou(bbox_cxcywh_to_xyxy(out_boxes), bbox_cxcywh_to_xyxy(tgt_bbox))\n",
    "    \n",
    "    # Final cost matrix\n",
    "    C = cost_bbox * cost_bbox_weight + cost_class * cost_class_weight + cost_giou * cost_giou_weight\n",
    "    C = C.view(num_queries, -1)\n",
    "    result = torch.argmin(C, axis=1)\n",
    "    return result\n",
    "\n",
    "def get_indexes(out_logits):\n",
    "    prob = out_logits.softmax(dim=1)\n",
    "    prob, _ = prob.max(dim=1)\n",
    "    select_mask = prob > score_threshold\n",
    "    if sum(select_mask) == 0:\n",
    "        score, indexes = prob.topk(10)\n",
    "        indexes,_ = torch.sort(indexes, dim=0)\n",
    "        print(f\"Cannot find detected objects with score larger than {score_threshold}\")\n",
    "    else:\n",
    "        indexes = select_mask.nonzero().reshape(-1)\n",
    "    return indexes\n",
    "\n",
    "def get_topk_indexes(out_logits, k):\n",
    "    k = min(k, out_logits.shape[0])\n",
    "    prob = out_logits.softmax(dim=1)\n",
    "    prob, _ = prob.max(dim=1)\n",
    "    score, indexes = prob.topk(k)\n",
    "    indexes,_ = torch.sort(indexes, dim=0)\n",
    "    return indexes\n",
    "\n",
    "def sigmoid_focal_loss(inputs, targets, alpha: float = 0.25, gamma: float = 2):\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = -1 (no weighting).\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "    Returns:\n",
    "        Loss tensor\n",
    "    \"\"\"\n",
    "    prob = inputs.sigmoid()\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "    return loss\n",
    "    \n",
    "def compute_loss(out_logits, out_boxes, tgt_ids, tgt_bbox, matched_target_indexes, cls_loss_coef = 1.0, bbox_loss_coef = 5.0, giou_loss_coef = 2.0):\n",
    "    if matched_target_indexes == None:\n",
    "        target_classes_onehot = torch.zeros([out_logits.shape[0], out_logits.shape[1]],dtype=out_logits.dtype, layout=out_logits.layout, \n",
    "                                        device=out_logits.device)\n",
    "        loss_ce = sigmoid_focal_loss(out_logits, target_classes_onehot)\n",
    "        loss_ce = loss_ce.sum(axis=1)\n",
    "        loss = loss_ce * cls_loss_coef\n",
    "        return loss, loss_ce, torch.zeros(loss.shape), torch.zeros(loss.shape)\n",
    "    cls_loss_coef = 1.0\n",
    "    bbox_loss_coef = 5.0\n",
    "    giou_loss_coef = 2.0\n",
    "    target_boxes = tgt_bbox[matched_target_indexes]\n",
    "    loss_bbox = F.l1_loss(out_boxes, target_boxes, reduction='none') # [num_queries, 4]\n",
    "    loss_bbox = loss_bbox.mean(axis=1) # [num_queries]\n",
    "    loss_giou = 1 - torch.diag(generalized_box_iou(bbox_cxcywh_to_xyxy(out_boxes),\n",
    "                bbox_cxcywh_to_xyxy(target_boxes))) # [num_queries]\n",
    "    target_classes_onehot = torch.zeros([out_logits.shape[0], out_logits.shape[1]],dtype=out_logits.dtype, layout=out_logits.layout, \n",
    "                                        device=out_logits.device)\n",
    "    target_labels = torch.tensor(tgt_ids)[matched_target_indexes]\n",
    "    target_classes_onehot.scatter_(1, target_labels.unsqueeze(-1), 1)\n",
    "    loss_ce = sigmoid_focal_loss(out_logits, target_classes_onehot)\n",
    "    loss_ce = loss_ce.sum(axis=1)\n",
    "    loss = loss_ce * cls_loss_coef + loss_bbox * bbox_loss_coef + loss_giou * giou_loss_coef\n",
    "    return loss, loss_ce, loss_bbox, loss_giou\n",
    "\n",
    "def generate_feature_annotation(num, count):\n",
    "    file_path = data_path + str(num) + \".json\"\n",
    "    output_data = read_json_results(file_path)\n",
    "    feature = np.array(output_data['feature'][5][0])\n",
    "    out_logits = torch.FloatTensor(output_data['pred_logits'][5][0])\n",
    "    out_boxes = torch.FloatTensor(output_data['pred_boxes'][5][0])\n",
    "    target_labels = output_data['gt_labels']\n",
    "    target_boxes = torch.FloatTensor(output_data['gt_boxes'])\n",
    "    if len(target_boxes) == 0:\n",
    "        return False\n",
    "    target_boxes = bbox_xyxy_to_cxcywh(target_boxes)\n",
    "    img_h, img_w = output_data['img_metas'][0]['img_shape']\n",
    "    factors = output_data['img_metas'][0]['scale_factor']\n",
    "    shapes = torch.FloatTensor([img_h, img_w, img_h, img_w])\n",
    "    target_boxes = target_boxes / shapes\n",
    "    out_logits = out_logits[:, 0:80] # for detr & coco\n",
    "    indexes = get_indexes(out_logits)\n",
    "    out_logits = out_logits[indexes]\n",
    "    out_boxes = out_boxes[indexes]\n",
    "    gt_indexes = hungarian_matching(out_logits, out_boxes, target_labels, target_boxes)\n",
    "    loss, loss_ce, loss_bbox, loss_giou = compute_loss(out_logits, out_boxes, target_labels, target_boxes, gt_indexes)\n",
    "    np_write(feature, feature_path + str(count) + \".npy\")\n",
    "    json_data = {}\n",
    "    json_data['loss'] = transform_tensors_to_list(loss)\n",
    "    json_data['loss_ce'] = transform_tensors_to_list(loss_ce)\n",
    "    json_data['loss_bbox'] = transform_tensors_to_list(loss_bbox)\n",
    "    json_data['loss_giou'] = transform_tensors_to_list(loss_giou)\n",
    "    json_data['index'] = transform_tensors_to_list(indexes)\n",
    "    json_data['gt_indexes'] = transform_tensors_to_list(gt_indexes)\n",
    "    json_data['output_file_num'] = num\n",
    "    write_json_results(json_data, annotation_path + str(count) + \".json\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbecfaad-ecc2-412c-a612-57430aae0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "base_path = \"./pro_data/DFDETR_COCO/\"\n",
    "data_path = base_path + split + \"/outputs/\"\n",
    "feature_path = base_path + split + \"/feature/\"\n",
    "annotation_path = base_path + split + \"/annotation/\"\n",
    "create_folder_if_not_exists(feature_path)\n",
    "create_folder_if_not_exists(annotation_path)\n",
    "score_threshold = 0.5 # DETR_COCO 0.98, DFDETR_COCO 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f42f5-7f0c-4668-a9ee-2cc7c4d0e21f",
   "metadata": {},
   "source": [
    "### Generate feature and annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e533913-9d46-40a7-a552-b4c0ebe2b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_folder = os.listdir(data_path)\n",
    "num_files = len(files_in_folder) - 1\n",
    "count = 0\n",
    "for num in range(num_files):\n",
    "    flag = generate_feature_annotation(num, count)\n",
    "    if flag:\n",
    "        count += 1\n",
    "    if (num+1)%1000 == 0:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a8661b-2715-4caf-ac2a-a16572beb4fd",
   "metadata": {},
   "source": [
    "### Generate image_true_losses and region_true_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09c7768f-df14-488b-a7df-d9da684aa19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_folder = os.listdir(annotation_path)\n",
    "nums = len(files_in_folder) - 1\n",
    "img_loss = np.zeros(nums)\n",
    "region_loss = []\n",
    "for i in range(nums):\n",
    "    path = annotation_path + str(i) + \".json\"\n",
    "    json_data = read_json_results(path)\n",
    "    losses = np.array(json_data['loss'])\n",
    "    img_loss[i] = losses.mean()\n",
    "    region_loss.extend(json_data['loss'])\n",
    "np_write(img_loss, base_path + split + \"/image_true_losses.npy\")\n",
    "np_write(np.array(region_loss), base_path + split + \"/region_true_losses.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eed780e8-4a89-4aab-8c5d-facb66b03a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8588651940226555 7.9476424853007\n",
      "0.23577462136745453 8.83905029296875\n"
     ]
    }
   ],
   "source": [
    "split = \"train\"\n",
    "image_loss = np_read(base_path + split + \"/image_true_losses.npy\")\n",
    "print(image_loss.min(), image_loss.max())\n",
    "region_loss = np_read(base_path + split + \"/region_true_losses.npy\")\n",
    "print(region_loss.min(), region_loss.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e58ed-5b98-40f8-8e2f-5b2e34400aa7",
   "metadata": {},
   "source": [
    "# change feature and output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a9e44d47-3657-41b0-9bde-ee6331c2e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "5999\n",
      "6999\n",
      "7999\n",
      "8999\n",
      "9999\n",
      "10999\n",
      "11999\n",
      "12999\n",
      "13999\n",
      "14999\n",
      "15999\n",
      "16999\n",
      "17999\n",
      "18999\n",
      "19999\n",
      "20999\n",
      "21999\n",
      "22999\n",
      "23999\n",
      "24999\n",
      "25999\n",
      "26999\n",
      "27999\n",
      "28999\n",
      "29999\n",
      "30999\n",
      "31999\n",
      "32999\n",
      "33999\n",
      "34999\n",
      "35999\n",
      "36999\n",
      "37999\n",
      "38999\n",
      "39999\n",
      "40999\n",
      "41999\n",
      "42999\n",
      "43999\n",
      "44999\n",
      "45999\n",
      "46999\n",
      "47999\n",
      "48999\n",
      "49999\n",
      "50999\n",
      "51999\n",
      "52999\n",
      "53999\n",
      "54999\n",
      "55999\n",
      "56999\n",
      "57999\n",
      "58999\n",
      "59999\n",
      "60999\n",
      "61999\n",
      "62999\n",
      "63999\n",
      "64999\n",
      "65999\n",
      "66999\n",
      "67999\n",
      "68999\n",
      "69999\n",
      "70999\n",
      "71999\n",
      "72999\n",
      "73999\n",
      "74999\n",
      "75999\n",
      "76999\n",
      "77999\n",
      "78999\n",
      "79999\n",
      "80999\n",
      "81999\n",
      "82999\n",
      "83999\n",
      "84999\n",
      "85999\n",
      "86999\n",
      "87999\n",
      "88999\n",
      "89999\n",
      "90999\n",
      "91999\n",
      "92999\n",
      "93999\n",
      "94999\n",
      "95999\n",
      "96999\n",
      "97999\n",
      "98999\n",
      "99999\n",
      "100999\n",
      "101999\n",
      "102999\n",
      "103999\n",
      "104999\n",
      "105999\n",
      "106999\n",
      "107999\n",
      "108999\n",
      "109999\n",
      "110999\n",
      "111999\n",
      "112999\n",
      "113999\n",
      "114999\n",
      "115999\n",
      "116999\n",
      "117999\n"
     ]
    }
   ],
   "source": [
    "files_in_folder = os.listdir(data_path)\n",
    "num_files = len(files_in_folder) - 1\n",
    "for num in range(num_files):\n",
    "    file_path = data_path + str(num) + \".json\"\n",
    "    output_data = read_json_results(file_path)\n",
    "    feature = output_data['feature'][5][0]\n",
    "    out_logits = output_data['pred_logits'][5][0]\n",
    "    out_boxes = output_data['pred_boxes'][5][0]\n",
    "    output_data['feature'] = feature\n",
    "    output_data['pred_logits'] = out_logits\n",
    "    output_data['pred_boxes'] = out_boxes\n",
    "    write_json_results(output_data, file_path)\n",
    "    if (num+1)%1000 == 0:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c803f9b-4013-4f59-a931-8dfb3ce3f232",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59ab38a0-2ede-412a-8e0f-90b52ae4176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "file_path = data_path + str(num) + \".json\"\n",
    "output_data = read_json_results(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de190d8e-dc73-4d8f-a2c9-eba489bd1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.array(output_data['feature'][5][0])\n",
    "out_logits = torch.FloatTensor(output_data['pred_logits'][5][0])\n",
    "out_boxes = torch.FloatTensor(output_data['pred_boxes'][5][0])\n",
    "out_logits = out_logits[:, 0:80] # for detr & coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a669b3a9-e14d-4585-9084-d6c2874ad7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indexes = get_topk_indexes(out_logits, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a9df0d5-ba5b-41fa-9e4b-6985c362f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature[selected_indexes]\n",
    "out_logits = out_logits[selected_indexes]\n",
    "out_boxes = out_boxes[selected_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25cf74b5-5649-4b3c-8b60-649f40fbb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = output_data['gt_labels']\n",
    "target_boxes = torch.FloatTensor(output_data['gt_boxes'])\n",
    "if len(target_boxes) > 0:\n",
    "    target_boxes = bbox_xyxy_to_cxcywh(target_boxes)\n",
    "img_h, img_w = output_data['img_metas'][0]['img_shape']\n",
    "factors = output_data['img_metas'][0]['scale_factor']\n",
    "shapes = torch.FloatTensor([img_h, img_w, img_h, img_w])\n",
    "target_boxes = target_boxes / shapes\n",
    "# out_logits = out_logits[:, 0:80] # for detr & coco\n",
    "indexes = get_indexes(out_logits)\n",
    "out_logits = out_logits[indexes]\n",
    "out_boxes = out_boxes[indexes]\n",
    "gt_indexes = hungarian_matching(out_logits, out_boxes, target_labels, target_boxes)\n",
    "loss, loss_ce, loss_bbox, loss_giou = compute_loss(out_logits, out_boxes, target_labels, target_boxes, gt_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19431ff3-7f63-423d-853a-6d785d336d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_write(feature, feature_path + str(num) + \".npy\")\n",
    "json_data = {}\n",
    "json_data['loss'] = transform_tensors_to_list(loss)\n",
    "json_data['loss_ce'] = transform_tensors_to_list(loss_ce)\n",
    "json_data['loss_bbox'] = transform_tensors_to_list(loss_bbox)\n",
    "json_data['loss_giou'] = transform_tensors_to_list(loss_giou)\n",
    "json_data['index'] = transform_tensors_to_list(indexes)\n",
    "json_data['gt_indexes'] = transform_tensors_to_list(gt_indexes)\n",
    "json_data['output_file_num'] = num\n",
    "json_data['img_metas'] = output_data['img_metas']\n",
    "write_json_results(json_data, annotation_path + str(num) + \".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "365d9023-bafd-41d8-940f-a9d2c6f6cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = read_json_results(base_path + \"train\" + \"/annotation/\" + \"55397.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8101dce2-2795-4790-b76a-0f7531b74892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55894"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json['output_file_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36c83fdd-f3f6-465f-8cb7-6311dc92338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_loss = torch.tensor(json['loss'])\n",
    "img_loss = patch_loss.mean()\n",
    "patch_index = torch.tensor(json['index'])\n",
    "annotation = torch.full((100,), 255, dtype=patch_loss.dtype)\n",
    "annotation[patch_index] = patch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a585024f-b39c-40cc-ab4e-3dca1df38a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([255.0000, 255.0000,   7.3523, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000,   4.2870, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   5.5946, 255.0000,\n",
       "          8.4145,   5.0837, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000,   5.2658,   5.4688,   5.0664,\n",
       "        255.0000, 255.0000,   5.0659, 255.0000,   7.0068, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000,   6.5768, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   8.6178,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,\n",
       "          4.8617, 255.0000, 255.0000,   6.8172, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000,   6.6019,   6.3947,  11.1883, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000, 255.0000, 255.0000, 255.0000, 255.0000,   5.1456,\n",
       "        255.0000, 255.0000, 255.0000,   6.7193, 255.0000, 255.0000, 255.0000,\n",
       "        255.0000, 255.0000])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a28bf2d-76ef-42ea-b683-cf0ac871aa75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 11, 19, 21, 22, 32, 33, 34, 37, 39, 51, 62, 70, 73, 78, 79, 80, 90,\n",
       "        94])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b99e9-3e52-4a04-b761-7e5b3ca8fbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
