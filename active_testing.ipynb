{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20e79cc-2909-4e00-9f29-5e0be5f27cc4",
   "metadata": {},
   "source": [
    "# Active testing for mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402a2f09-f3e3-40d5-859d-2c7b56ca50f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from tools.utils_at import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feeb3ff5-332d-4b2a-8139-80f65cde9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LURE_weights_for_risk_estimator(weights, N):\n",
    "    M = weights.size\n",
    "    if M < N:\n",
    "        m = np.arange(1, M+1)\n",
    "        v = (\n",
    "            1\n",
    "            + (N-M)/(N-m) * (\n",
    "                    1 / ((N-m+1) * weights)\n",
    "                    - 1\n",
    "                    )\n",
    "            )\n",
    "    else:\n",
    "        v = 1\n",
    "\n",
    "    return v\n",
    "\n",
    "def acquire(expected_loss_inputs, samples_num):\n",
    "    assert samples_num <= expected_loss_inputs.size\n",
    "    expected_loss = np.copy(expected_loss_inputs)\n",
    "    # Log-lik can be negative.\n",
    "    # Make all values positive.\n",
    "    if (expected_loss < 0).sum() > 0:\n",
    "        expected_loss += np.abs(expected_loss.min())\n",
    "    \n",
    "    if np.any(np.isnan(expected_loss)):\n",
    "        logging.warning(\n",
    "            'Found NaN values in expected loss, replacing with 0.')\n",
    "        logging.info(f'{expected_loss}')\n",
    "        expected_loss = np.nan_to_num(expected_loss, nan=0)\n",
    "    pick_sample_idxs = np.zeros((samples_num), dtype = int)\n",
    "    idx_array = np.arange(expected_loss.size)\n",
    "    weights = np.zeros((samples_num), dtype = np.single)\n",
    "    uniform_clip_val = 0.2\n",
    "    expected_loss = np.asarray(expected_loss).astype('float64')\n",
    "    for i in range(samples_num):\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        # clip all values less than 10 percent of uniform propability\n",
    "        expected_loss = np.maximum(uniform_clip_val * 1/expected_loss.size, expected_loss)\n",
    "        expected_loss /= expected_loss.sum()\n",
    "        sample = np.random.multinomial(1, expected_loss)\n",
    "        cur_idx = np.where(sample)[0][0]\n",
    "        # cur_idx = np.random.randint(expected_loss.size)\n",
    "        pick_sample_idxs[i] = idx_array[cur_idx]\n",
    "        weights[i] = expected_loss[cur_idx]\n",
    "        selected_mask = np.ones((expected_loss.size), dtype=bool)\n",
    "        selected_mask[cur_idx] = False\n",
    "        expected_loss = expected_loss[selected_mask]\n",
    "        idx_array = idx_array[selected_mask]\n",
    "    return pick_sample_idxs, weights\n",
    "\n",
    "def active_testing(file_path, true_losses, expected_losses, active_test_type, display = False, store_idxs = False):\n",
    "    json_object = {}\n",
    "    for seed in random_seed_set:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        pick_sample_idxs, weights = acquire(expected_losses, sample_size_set[-1])\n",
    "        if store_idxs:\n",
    "            np_write(pick_sample_idxs, result_json_path + f\"{seed}_at_idxs.npy\")\n",
    "        for sample_size in sample_size_set:\n",
    "            result = {\"active_test_type\": active_test_type, \"sample_size\": sample_size}\n",
    "            risk_estimator_weights = LURE_weights_for_risk_estimator(weights[:sample_size], expected_losses.size)\n",
    "            sampled_true_losses = true_losses[pick_sample_idxs[:sample_size]]\n",
    "            loss_risk = (sampled_true_losses * risk_estimator_weights).mean()\n",
    "            result[\"loss\"] = loss_risk\n",
    "            json_object[len(json_object)] = result\n",
    "        if display:\n",
    "            print(f\"Complete seed : {seed}\")\n",
    "    with open(file_path, \"w\") as outfile:\n",
    "        json.dump(json_object, outfile)\n",
    "\n",
    "def get_whole_data_set_risk_estimator(true_losses):\n",
    "    return float(true_losses.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cadfb7-0dd4-40b7-826d-2c5b0b137dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "model_data_type = \"DFDETR_COCO\"\n",
    "base_path = f\"./pro_data/{model_data_type}/{split}\"\n",
    "annotation_path = base_path + \"/annotation/\"\n",
    "data_type = \"image\"\n",
    "result_json_path = f\"./results/{model_data_type}/\"\n",
    "create_folder_if_not_exists(result_json_path)\n",
    "if data_type == \"image\":\n",
    "    result_json_path = result_json_path + \"image_based_active_testing/\"\n",
    "    true_losses = np_read(base_path + \"/image_true_losses.npy\")\n",
    "    sample_size_precentage = np.linspace(0.002, 0.2, 500)\n",
    "elif data_type == \"region\":\n",
    "    result_json_path = result_json_path + \"region_based_active_testing/\"\n",
    "    true_losses = np_read(base_path + \"/region_true_losses.npy\")\n",
    "    sample_size_precentage = np.linspace(0.001, 0.2, 500)\n",
    "labels_nums = true_losses.shape[0]\n",
    "sample_size_set = (np.array(sample_size_precentage) * labels_nums).astype(int).tolist()\n",
    "vit_base_path = \"../ViT-pytorch/output/\"\n",
    "create_folder_if_not_exists(result_json_path)\n",
    "random_seed_set = [4519, 9524, 5901]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e3d64-9916-44e5-b0e8-a69149878ab3",
   "metadata": {},
   "source": [
    "## Random Sample Risk Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119abd18-eb2e-4d2b-95d3-2837a7b199c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"random_sample_3_runs.json\"\n",
    "json_object = {}\n",
    "for seed in random_seed_set:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    perm = np.random.permutation(true_losses.size)\n",
    "    samples_num = sample_size_set[-1]\n",
    "    pick_sample_idxs = perm[:samples_num]\n",
    "    if False:\n",
    "        np_write(pick_sample_idxs, result_json_path+f\"{seed}_rs_idxs.npy\")\n",
    "    for sample_size in sample_size_set:\n",
    "        result = {\"active_test_type\": \"random sample\", \"sample_size\": sample_size}\n",
    "        loss_risk = true_losses[pick_sample_idxs[:sample_size]].mean()\n",
    "        result[\"loss\"] = float(loss_risk)\n",
    "        json_object[len(json_object)] = result\n",
    "write_json_results(json_object, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e4a5c-06b6-454c-bf52-a981211dd029",
   "metadata": {},
   "source": [
    "## Whole data set risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e88dc56-8fca-4e8b-856e-f3259a02a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = result_json_path + \"None.json\"\n",
    "result = {\"active_test_type\": \"None\", \"sample_size\": true_losses.size}\n",
    "result[\"loss\"] = get_whole_data_set_risk_estimator(true_losses)\n",
    "json_object = {}\n",
    "json_object[0] = result\n",
    "write_json_results(json_object, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd853c-ae92-461f-bcaa-ad2d74645aa0",
   "metadata": {},
   "source": [
    "## ViT active testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43424f-3fee-4018-af1a-6fb88ac116d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = 10000\n",
    "val_estimated_loss = np.array(read_json_results(f\"{vit_base_path}/ViT_{model_data_type}_all_{data_type}_losses_{train_step}.json\")['losses'])\n",
    "file_path = result_json_path + f\"ViT_all_runs_{train_step}.json\"\n",
    "active_testing(file_path, true_losses, val_estimated_loss, \"ViT all\", store_idxs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87cd4040-d7a5-486e-9282-2984a998f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = np.linspace(10000, 100000, 10, dtype=int)\n",
    "# train_steps = np.linspace(10000, 50000, 5, dtype=int)\n",
    "for train_step in train_steps:\n",
    "    val_estimated_loss = np.array(read_json_results(f\"{vit_base_path}/ViT_{model_data_type}_all_{data_type}_losses_{train_step}.json\")['losses'])\n",
    "    file_path = result_json_path + f\"ViT_all_runs_{train_step}.json\"\n",
    "    active_testing(file_path, true_losses, val_estimated_loss, \"ViT all\", store_idxs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395fca-bc79-4e6d-a0db-1e8c61c9733b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
